{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'utils')))  # Adjust the path based on your structure\n",
    "from logging_configuration import setup_logging\n",
    "from load_from_csv import load_data\n",
    "from tabulate_style import tab_fmt\n",
    "from custom_plotly_template import get_custom_layout, set_custom_template\n",
    "\n",
    "# Call the function to set the custom template\n",
    "set_custom_template() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_paragraphs(df, lower_bound=0, upper_bound=float('inf'), keywords=None, return_type='dataframe'):\n",
    "    \"\"\"\n",
    "    Analyzes paragraphs for token distribution and can return data or plots.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing paragraphs.\n",
    "    - lower_bound: int, minimum token count to include.\n",
    "    - upper_bound: int, maximum token count to include.\n",
    "    - keywords: list of str, specific tokens to filter.\n",
    "    - return_type: str, 'dataframe', 'plot', or 'both' to specify output preference.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame of token counts or None based on return_type.\n",
    "    \"\"\"\n",
    "    # Explode the 'paragraph' column into tokens\n",
    "    tokens_df = df['paragraph'].str.split(' ').explode().reset_index(drop=True)\n",
    "\n",
    "    # Count the total number of tokens\n",
    "    total_tokens = tokens_df.count()\n",
    "\n",
    "    # Calculate the number of tokens in each paragraph\n",
    "    token_counts = df['paragraph'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Combine the lengths and token counts into a single DataFrame\n",
    "    stats_df = pd.DataFrame({'token_count': token_counts})\n",
    "\n",
    "    # Filter token counts based on provided lower and upper bounds\n",
    "    filtered_stats_df = stats_df[(stats_df['token_count'] >= lower_bound) & \n",
    "                                  (stats_df['token_count'] <= upper_bound)]\n",
    "\n",
    "    # Create a DataFrame for token counts\n",
    "    token_count_series = tokens_df.value_counts().reset_index(name='count')\n",
    "    token_count_series.columns = ['token', 'count']  # Rename columns\n",
    "\n",
    "    # If keywords are provided, filter the token counts\n",
    "    if keywords is not None:\n",
    "        token_count_series = token_count_series[token_count_series['token'].isin(keywords)]\n",
    "\n",
    "    # Filter token counts based on provided lower and upper bounds\n",
    "    token_count_series = token_count_series[(token_count_series['count'] >= lower_bound) & \n",
    "                                            (token_count_series['count'] <= upper_bound)]\n",
    "\n",
    "    # Logging for filtered data\n",
    "    #tab_fmt(filtered_stats_df, 5, style='psql')\n",
    "    logging.info(f\"Top 5 values within the provided range\")\n",
    "    tab_fmt(token_count_series, 5, style='psql')\n",
    "\n",
    "    #logging.info(f\"Filtered stats_df:\\n{tabulate(filtered_stats_df, headers='keys', tablefmt='psql')}\")\n",
    "    #logging.info(f\"Filtered token_count_series:\\n{tabulate(token_count_series, headers='keys', tablefmt='psql')}\")\n",
    "\n",
    "    # Check for empty data\n",
    "    if filtered_stats_df.empty:\n",
    "        logging.warning(\"No data available for the specified token count range in filtered_stats_df.\")\n",
    "    if token_count_series.empty:\n",
    "        logging.warning(\"No data available for the specified token count range in token_count_series.\")\n",
    "        return pd.DataFrame() if return_type in ['dataframe', 'both'] else None  # Return empty DataFrame if no data is available\n",
    "\n",
    "    # Calculate the 95th percentile and round to 2 decimal places\n",
    "    percentile_95 = round(token_count_series['count'].quantile(0.95), 2)\n",
    "\n",
    "    # Identify the token corresponding to the 95th percentile\n",
    "    token_95 = token_count_series[token_count_series['count'] >= percentile_95]['token'].iloc[0]\n",
    "\n",
    "    # Log the 95th percentile token\n",
    "    logging.info(f\"95th Percentile Token: {token_95} with Count: {percentile_95}\")\n",
    "\n",
    "    # Prepare to return plots\n",
    "    plots = []\n",
    "    if return_type in ['plot', 'both']:\n",
    "        # Plot histogram of token counts\n",
    "        fig_tokens = px.histogram(filtered_stats_df, x='token_count', nbins=50, \n",
    "                                   color='token_count',\n",
    "                                   title='Distribution of Token Counts',\n",
    "                                   labels={'token_count': 'Token Count', 'count': 'Count'})\n",
    "        plots.append(fig_tokens)\n",
    "\n",
    "        # Plot bar chart for token counts\n",
    "        fig_tokens_bar = px.bar(token_count_series, x='token', y='count',\n",
    "                                 title='Token Counts',\n",
    "                                 labels={'token': 'Token', 'count': 'Count'},\n",
    "                                 color='count')\n",
    "\n",
    "        # Add 95th percentile line\n",
    "        fig_tokens_bar.add_trace(go.Scatter(\n",
    "            x=token_count_series['token'],\n",
    "            y=[percentile_95] * len(token_count_series),\n",
    "            mode='lines',\n",
    "            name='95th Percentile',\n",
    "            line=dict(color='red', width=2, dash='dash')\n",
    "        ))\n",
    "        plots.append(fig_tokens_bar)\n",
    "\n",
    "    # Show the plots if required\n",
    "    if return_type in ['plot', 'both']:\n",
    "        for plot in plots:\n",
    "            plot.show()\n",
    "\n",
    "    logging.info(f\"Total tokens extracted: {total_tokens}\")\n",
    "    logging.info(\"Paragraph analysis completed!\")\n",
    "\n",
    "    return token_count_series if return_type in ['dataframe', 'both'] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================================================================================================#\n",
    "#== Preview the main dataframe ==#\n",
    "#df = load_data()\n",
    "#tab_fmt(df, 5, style='psql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For only DataFrame output\n",
    "#token_counts = analyze_paragraphs(df, lower_bound=50, upper_bound=1500, keywords=None, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_paragraphs(df, lower_bound=70, upper_bound=1300, keywords=None, return_type='plot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_gemma-dF4c2h5V",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
