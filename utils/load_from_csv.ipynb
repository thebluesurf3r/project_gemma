{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 15:43:49,287 - INFO - Logging initialized and set up successfully.\n",
      "2024-10-27 15:43:49,291 - INFO - Viewing the log: <function display_log_df at 0x7458fdfe6e60>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [timestamp, level, message]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Import custom logging configuration and log DataFrame\n",
    "from logging_configuration import setup_logging\n",
    "\n",
    "#=====================================================================================================================================================#\n",
    "#== Configure logging with specified log file and level ==#\n",
    "\n",
    "logger = setup_logging(log_file='app_log.log', log_level=logging.INFO)\n",
    "\n",
    "#=====================================================================================================================================================#\n",
    "#== Function to Load Dataset from CSV File ==#\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load a CSV dataset from the specified file path and log details about the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The loaded DataFrame, or None if loading fails.\n",
    "    \"\"\"\n",
    "    file_name = \"cagliostro_gutenberg.csv\"  # Specify the CSV file name\n",
    "    file_path = os.path.join(\"..\", \"csv\", file_name)  # Construct the full file path\n",
    "\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")  # Log error if file is missing\n",
    "            return None\n",
    "\n",
    "        # Load the dataset into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the DataFrame is empty\n",
    "        if df.empty:\n",
    "            logger.warning(f\"{file_name} is empty. No data to load.\")  # Log warning for empty DataFrame\n",
    "            return None\n",
    "\n",
    "        # Log successful import of the dataset\n",
    "        logger.info(f\"{file_name} imported successfully!\")\n",
    "        logger.info(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")  # Log DataFrame dimensions\n",
    "\n",
    "        # Export schema of the DataFrame\n",
    "        schema = df.dtypes.reset_index()  # Get data types and reset index\n",
    "        schema.columns = ['Column Name', 'Data Type']  # Rename columns for clarity\n",
    "\n",
    "        # Get unique values for each column\n",
    "        unique_values = df.nunique()  # Count unique values in each column\n",
    "        schema['n_unique'] = unique_values.values  # Add unique counts to the schema DataFrame\n",
    "\n",
    "        # Log the schema of the loaded dataset\n",
    "        logger.info(\"Schema of the loaded dataset:\")\n",
    "        logger.info(f\"\\n{tabulate(schema, headers='keys', tablefmt='psql')}\")  # Log formatted schema\n",
    "\n",
    "        # Handle columns with zero unique values\n",
    "        zero_unique_cols = schema[schema['n_unique'] == 0]['Column Name'].tolist()\n",
    "        for col in zero_unique_cols:\n",
    "            logger.warning(f\"Column '{col}' has no unique values and will be dropped.\")  # Log warning\n",
    "            df.drop(columns=col, inplace=True)  # Drop the column\n",
    "\n",
    "        return df  # Return the loaded DataFrame\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(\"The file is empty or contains no data.\")  # Log error for empty file\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(\"Error parsing the CSV file. Please check the file format.\")  # Log error for parsing issues\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {str(e)}\")  # Log any other unexpected errors\n",
    "\n",
    "    # Return None if any error occurs\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================================================================================================#\n",
    "#== Load the Data and Store it in a DataFrame ==#\n",
    "\n",
    "#df = load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_gemma-dF4c2h5V",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
