{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Level Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from logging.handlers import RotatingFileHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 20:42:01,543 - INFO - Logging initialized and set up successfully.\n",
      "2024-10-27 20:42:01,548 - INFO - Log size: (0, 3)\n",
      "2024-10-27 20:42:01,552 - INFO - Viewing the log: <function display_log_df at 0x7afb6158ba30>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp, level, message]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================================================================================================#\n",
    "#== Function level imports ==#\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from sqlalchemy import create_engine, Table, Column, MetaData, String, DateTime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from datetime import datetime\n",
    "\n",
    "#== Defines a global dataframe to store log entries ==#\n",
    "\n",
    "log_df = pd.DataFrame(columns=['timestamp', 'level', 'message'])\n",
    "\n",
    "#== Defines a class for logging handler ==#\n",
    "\n",
    "class DataFrameLoggingHandler(logging.Handler):\n",
    "    def __init__(self, dataframe, queue):\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.queue = queue\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            timestamp = pd.Timestamp.now()\n",
    "            level = record.levelname\n",
    "            message = record.getMessage()\n",
    "            self.queue.put((timestamp, level, message))\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "#== Defines a function to process log queue ==#\n",
    "\n",
    "def process_log_queue(dataframe, queue, push_interval=10):\n",
    "    \"\"\"\n",
    "    Processes the log queue, adding new logs to the DataFrame and pushing to the database.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while not queue.empty():\n",
    "            timestamp, level, message = queue.get()\n",
    "            dataframe.loc[len(dataframe)] = [timestamp, level, message]\n",
    "        \n",
    "        # Push to PostgreSQL at regular intervals\n",
    "        if len(dataframe) > 0:\n",
    "            push_to_database(dataframe)\n",
    "            dataframe.drop(dataframe.index, inplace=True)  # Clear DataFrame after pushing\n",
    "\n",
    "        time.sleep(push_interval)\n",
    "\n",
    "#== Evaluates whether the table already exists in the database ==#\n",
    "\n",
    "def create_table_if_not_exists(engine):\n",
    "    \"\"\"\n",
    "    Create the table 'website_log' in the 'light_site' database if it doesn't already exist,\n",
    "    ensuring the column names are lowercase for compatibility with PostgreSQL.\n",
    "    \"\"\"\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Define table schema with lowercase column names\n",
    "    website_log = Table(\n",
    "        'website_log', metadata,\n",
    "        Column('timestamp', DateTime, nullable=False),  # Lowercase column names\n",
    "        Column('level', String, nullable=False),\n",
    "        Column('message', String, nullable=False)\n",
    "    )\n",
    "\n",
    "    # Create the table if it does not exist\n",
    "    metadata.create_all(engine)\n",
    "\n",
    "#== Pushes the log data to the database table ==#\n",
    "\n",
    "def push_to_database(dataframe):\n",
    "    \"\"\"\n",
    "    Pushes the log DataFrame to the PostgreSQL database, ensuring lowercase columns,\n",
    "    and creates the table if it doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define your PostgreSQL connection string\n",
    "        engine = create_engine('postgresql://postgres:password@localhost:5432/project_gemma')\n",
    "\n",
    "        # Ensure lowercase column names in DataFrame\n",
    "        dataframe.columns = [col.lower() for col in dataframe.columns]\n",
    "\n",
    "        # Create the table 'website_log' if it doesn't exist\n",
    "        create_table_if_not_exists(engine)\n",
    "\n",
    "        # Insert DataFrame into the table 'website_log' (append mode)\n",
    "        dataframe.to_sql('website_log', engine, if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error pushing data to PostgreSQL: {str(e)}\")\n",
    "\n",
    "#== Logging setup ==#\n",
    "\n",
    "def setup_logging(log_file='app_log.log', log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Set up logging configuration to log to a file, console, and DataFrame, and return a logger.\n",
    "    \"\"\"\n",
    "    logging.root.handlers.clear()\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # Define handlers\n",
    "    log_queue = Queue()\n",
    "    handlers = [\n",
    "        RotatingFileHandler(log_file, maxBytes=10485760, backupCount=5),  # 10 MB per file\n",
    "        logging.StreamHandler(),\n",
    "        DataFrameLoggingHandler(log_df, log_queue)\n",
    "    ]\n",
    "    \n",
    "    for handler in handlers:\n",
    "        handler.setLevel(log_level)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(handler)\n",
    "\n",
    "    # Start a thread to process the log queue and push to the database\n",
    "    log_thread = Thread(target=process_log_queue, args=(log_df, log_queue), daemon=True)\n",
    "    log_thread.start()\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Usage\n",
    "logger = setup_logging(log_file='app_log.log', log_level=logging.INFO)\n",
    "logger.info(\"Logging initialized and set up successfully.\")\n",
    "\n",
    "#== Function to Display Website Log ==#\n",
    "\n",
    "logging.info(f\"Log size: {log_df.shape}\")\n",
    "\n",
    "def display_log_df(log_df, rows=5, exclude_column=None):\n",
    "    logging.info(f\"Viewing the log: {display_log_df}\")\n",
    "    if exclude_column in log_df.columns:\n",
    "        log_df = log_df.drop(columns=exclude_column)\n",
    "    return log_df.head(rows)\n",
    "\n",
    "preview_log = display_log_df(log_df, rows=20)\n",
    "preview_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize paths for output parquet file for the main dataframe that is being analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 20:42:01,993 - INFO - Base directory initialized: /home/tron/git/project_gemma\n",
      "2024-10-27 20:42:01,995 - INFO - JDBC directory initialized: /home/tron/git/project_gemma/jdbc\n",
      "2024-10-27 20:42:02,000 - INFO - Output path initialized: /home/tron/git/project_gemma/jdbc/output.parquet\n"
     ]
    }
   ],
   "source": [
    "def initialize_paths():\n",
    "    \"\"\"\n",
    "    Initializes paths for output and logs the process.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing base_dir, jdbc_dir, and output_path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize base directory\n",
    "        base_dir = os.path.join(os.path.expanduser('~'), 'git', 'project_gemma')\n",
    "        logging.info(f\"Base directory initialized: {base_dir}\")\n",
    "        \n",
    "        # Initialize JDBC directory\n",
    "        jdbc_dir = os.path.join(base_dir, \"jdbc\")\n",
    "        logging.info(f\"JDBC directory initialized: {jdbc_dir}\")\n",
    "        \n",
    "        # Initialize output path\n",
    "        output_path = os.path.join(jdbc_dir, \"output.parquet\")\n",
    "        logging.info(f\"Output path initialized: {output_path}\")\n",
    "        \n",
    "        return base_dir, jdbc_dir, output_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing paths: {str(e)}\")\n",
    "        raise  # Re-raise the exception after logging\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    base_dir, jdbc_dir, output_path = initialize_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup database and define table structure for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, TIMESTAMP\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base  # Updated import\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = 'postgresql://postgres:password@localhost:5432/project_gemma'  # Update with your credentials\n",
    "engine = create_engine(DATABASE_URL)\n",
    "Base = declarative_base()  # This line remains unchanged\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Define the table structure\n",
    "class ProjectGutenberg(Base):\n",
    "    __tablename__ = 'cagliostro_gutenberg'  # Use the correct table name\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    chapter_title = Column(String(255), nullable=True)  # Changed from title to chapter_title\n",
    "    paragraph = Column(Text, nullable=True)              # Added paragraph field\n",
    "    quote = Column(Text, nullable=True)                  # Added quote field\n",
    "    source_url = Column(String(255), nullable=True)     # Added source_url field\n",
    "    created_at = Column(TIMESTAMP, nullable=True)        # Added created_at field\n",
    "    title = Column(String(255), nullable=True)           # Added title field\n",
    "    content = Column(Text, nullable=True)                 # Content can remain as is\n",
    "\n",
    "# Create the table in the database (if it doesn't already exist)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining web scraping logic and pushing the scraped content to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 20:42:06,067 - INFO - Data from https://www.forbes.com/sites/alisondurkee/2024/10/25/america-first-agenda-what-to-know-about-the-project-2025-alternative-that-trump-isnt-disavowing/? has been successfully scraped and stored.\n"
     ]
    }
   ],
   "source": [
    "def scrape_html(html_url):\n",
    "    # Fetch the HTML content\n",
    "    response = requests.get(html_url)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract relevant data\n",
    "    chapters = []\n",
    "    title = soup.find('h1').get_text(strip=True)  # Assuming the main title is in <h1>\n",
    "    chapter_titles = soup.find_all('h2')  # Assuming chapter titles are in <h2>\n",
    "    \n",
    "    for chapter in chapter_titles:\n",
    "        chapter_title_text = chapter.get_text(strip=True)\n",
    "        paragraphs = chapter.find_next_siblings('p')  # Get paragraphs after the chapter title\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # Stop when we hit the next chapter title\n",
    "            if paragraph.name == 'h2':\n",
    "                break\n",
    "            \n",
    "            paragraph_text = paragraph.get_text(strip=True)\n",
    "            chapters.append({\n",
    "                'chapter_title': chapter_title_text,\n",
    "                'paragraph': paragraph_text,\n",
    "                'quote': None,  # Handle quotes separately if needed\n",
    "                'source_url': html_url,\n",
    "                'title': title,\n",
    "            })\n",
    "\n",
    "    # Insert data into the database\n",
    "    for chapter in chapters:\n",
    "        db_entry = ProjectGutenberg(\n",
    "            chapter_title=chapter['chapter_title'],\n",
    "            paragraph=chapter['paragraph'],\n",
    "            quote=chapter['quote'],\n",
    "            source_url=chapter['source_url'],\n",
    "            title=chapter['title'],\n",
    "        )\n",
    "        session.add(db_entry)\n",
    "\n",
    "    session.commit()\n",
    "    logging.info(f\"Data from {html_url} has been successfully scraped and stored.\")\n",
    "\n",
    "# Example usage\n",
    "html_url = 'https://www.forbes.com/sites/alisondurkee/2024/10/25/america-first-agenda-what-to-know-about-the-project-2025-alternative-that-trump-isnt-disavowing/?'  # Replace with the actual HTML URL\n",
    "scrape_data = scrape_html(html_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_gemma-dF4c2h5V",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
