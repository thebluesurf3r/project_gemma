{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23c1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b8ce9d-491a-406c-b747-ad7454cc5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "import toml\n",
    "\n",
    "# Load secrets from .streamlit/secrets.toml\n",
    "def load_secrets():\n",
    "    return toml.load('.streamlit/secrets.toml')\n",
    "\n",
    "# Function to fetch and parse HTML\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape historical text data\n",
    "def scrape_historical_text(url):\n",
    "    html = fetch_html(url)\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract chapter titles (assuming they are in h2 tags)\n",
    "        chapters = [h2.get_text(strip=True).replace('\\n', ' ') for h2 in soup.find_all('h2')]\n",
    "\n",
    "        # Extract paragraphs (assuming they are in p tags)\n",
    "        paragraphs = [p.get_text(strip=True).replace('\\n', ' ') for p in soup.find_all('p')]\n",
    "\n",
    "        # Extract blockquote (quoted texts or poems)\n",
    "        quotes = [blockquote.get_text(strip=True).replace('\\n', ' ') for blockquote in soup.find_all('blockquote')]\n",
    "\n",
    "        return {\n",
    "            \"chapters\": chapters,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"quotes\": quotes\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Function to insert scraped data into PostgreSQL\n",
    "def insert_into_db(data, url):\n",
    "    secrets = load_secrets()\n",
    "\n",
    "    try:\n",
    "        # Connect to PostgreSQL using credentials from secrets.toml\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=secrets['database']['name'],\n",
    "            user=secrets['database']['user'],\n",
    "            password=secrets['database']['password'],\n",
    "            host=secrets['database']['host'],\n",
    "            port=secrets['database']['port']\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into the table\n",
    "        for chapter, paragraph, quote in zip(data['chapters'], data['paragraphs'], data['quotes']):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO cagliostro_gutenberg (chapter_title, paragraph, quote, source_url)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "            \"\"\", (chapter, paragraph, quote, url))\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "\n",
    "        # Close the connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert data: {e}\")\n",
    "\n",
    "# Example usage\n",
    "url = 'https://www.gutenberg.org/cache/epub/74618/pg74618-images.html'\n",
    "historical_data = scrape_historical_text(url)\n",
    "\n",
    "if historical_data:\n",
    "    insert_into_db(historical_data, url)\n",
    "else:\n",
    "    print(\"No data scraped from the URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a716d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize paths for output\n",
    "base_dir = os.path.join(os.path.expanduser('~'), 'git', 'automation_project', 'email_automation')\n",
    "jdbc_dir = os.path.join(base_dir, \"jbdc\")\n",
    "output_path = os.path.join(jdbc_dir, \"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba31459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 22:42:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- chapter_title: string (nullable = true)\n",
      " |-- paragraph: string (nullable = true)\n",
      " |-- quote: string (nullable = true)\n",
      " |-- source_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "| id|       chapter_title|        paragraph|               quote|          source_url|          created_at|\n",
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|  1|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-22 09:29:...|\n",
      "|  2|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-22 09:52:...|\n",
      "|  3|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-22 09:54:...|\n",
      "|  4|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-22 09:59:...|\n",
      "|  5|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 11:30:...|\n",
      "|  6|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 21:15:...|\n",
      "|  7|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 21:36:...|\n",
      "|  8|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 21:55:...|\n",
      "|  9|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 21:56:...|\n",
      "| 10|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 22:32:...|\n",
      "| 11|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 22:35:...|\n",
      "| 12|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 22:36:...|\n",
      "| 13|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 22:37:...|\n",
      "| 14|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-25 22:42:...|\n",
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Create Spark session with PostgreSQL JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Postgres_Spark_Analysis\") \\\n",
    "    .config(\"spark.jars\", \"/home/tron/git/project_gemma/jdbc/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define JDBC URL and properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/project_gemma\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Step 3: Load data from the PostgreSQL table into a Spark DataFrame\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"cagliostro_gutenberg\", properties=db_properties)\n",
    "\n",
    "# Step 4: Show the DataFrame schema and sample data\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65fb54c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "| id|       chapter_title|        paragraph|               quote|          source_url|          created_at|\n",
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|  1|The Project Guten...|Title: Cagliostro|This eBook is for...|https://www.guten...|2024-10-22 09:29:...|\n",
      "+---+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a row index column\n",
    "df_with_index = df.withColumn(\"row_idx\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out rows with indices 1 to 14\n",
    "df_filtered = df_with_index.filter(~((df_with_index.row_idx >= 1) & (df_with_index.row_idx <= 14)))\n",
    "\n",
    "# Drop the temporary row index column\n",
    "df_filtered = df_filtered.drop(\"row_idx\")\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80a0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "df_pandas = df_filtered.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b795e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 22:44:03,248 - INFO - DataFrame Shape: (1, 6)\n",
      "2024-10-25 22:44:03,256 - INFO - Truncated DataFrame View:\n",
      "+----+------+------------------------------------------+-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------+\n",
      "|    |   id | chapter_title                            | paragraph         | quote                                                                       | source_url                                                     | created_at                 |\n",
      "|----+------+------------------------------------------+-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------|\n",
      "|  0 |    1 | The Project Gutenberg eBook ofCagliostro | Title: Cagliostro | This eBook is for the use of anyone anywhere in the United States and most  | https://www.gutenberg.org/cache/epub/74618/pg74618-images.html | 2024-10-22 09:29:52.757358 |\n",
      "|    |      |                                          |                   |      other parts of the world at no cost and with almost no restrictions    |                                                                |                            |\n",
      "|    |      |                                          |                   |      whatsoever. You may copy it, give it away or re-use it under the terms |                                                                |                            |\n",
      "|    |      |                                          |                   |      of the Project Gutenberg License included with this eBook or online    |                                                                |                            |\n",
      "|    |      |                                          |                   |      atwww.gutenberg.org. If you                                            |                                                                |                            |\n",
      "|    |      |                                          |                   |      are not located in the United States, you will have to check the laws  |                                                                |                            |\n",
      "|    |      |                                          |                   |      of the country where you are located before using this eBook.          |                                                                |                            |\n",
      "+----+------+------------------------------------------+-------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tabulate import tabulate\n",
    "\n",
    "def data_overview(df_pandas):\n",
    "    # Display first row as a sample\n",
    "    df_truncated_view = df_pandas.head(1)\n",
    "    \n",
    "    # Get the shape of the DataFrame\n",
    "    df_shape = df_pandas.shape\n",
    "    \n",
    "    # Log the DataFrame shape\n",
    "    logging.info(f\"DataFrame Shape: {df_shape}\")\n",
    "    \n",
    "    # Log the tabulated view of the first row\n",
    "    logging.info(\"Truncated DataFrame View:\\n\" + tabulate(df_truncated_view, headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "\n",
    "data_overview(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9907b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tron/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "24/10/25 22:47:20 ERROR Executor: Exception in task 2.0 in stage 7.0 (TID 12) 3]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_14677/1090408783.py\", line 31, in tokenize\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/tron/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/share/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/10/25 22:47:20 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 12) (tron-01.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_14677/1090408783.py\", line 31, in tokenize\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/tron/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/share/nltk_data'\n",
      "    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/10/25 22:47:20 ERROR TaskSetManager: Task 2 in stage 7.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_14677/1090408783.py\", line 31, in tokenize\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/tron/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/share/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, sentiment_udf(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquote\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Show Results\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquote\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenized_quote\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnltk_sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/pyspark/sql/dataframe.py:978\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    971\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    972\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         },\n\u001b[1;32m    976\u001b[0m     )\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_14677/1090408783.py\", line 31, in tokenize\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n  File \"/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/python3.10/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/tron/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/share/nltk_data'\n    - '/home/tron/.local/share/virtualenvs/project_gemma-dF4c2h5V/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text Analysis with PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', force=True)  # Re-download punkt\n",
    "nltk.download('vader_lexicon', force=True)  # Re-download vader_lexicon\n",
    "\n",
    "\n",
    "# Initialize NLTK Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample data to create the DataFrame\n",
    "data = [\n",
    "    (1, \"The Project Gutenberg eBook of Cagliostro\", \n",
    "     \"This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"chapter_title\", \"quote\"])\n",
    "\n",
    "# Step 1: Define UDF for tokenization\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "tokenize_udf = udf(tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Step 2: Tokenize the 'quote' column\n",
    "df = df.withColumn('tokenized_quote', tokenize_udf(df['quote']))\n",
    "\n",
    "# Step 3: Define UDF for sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "sentiment_udf = udf(get_sentiment, StringType())\n",
    "\n",
    "# Step 4: Analyze sentiment\n",
    "df = df.withColumn('nltk_sentiment', sentiment_udf(df['quote']))\n",
    "\n",
    "# Show Results\n",
    "df.select('id', 'quote', 'tokenized_quote', 'nltk_sentiment').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5996e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
